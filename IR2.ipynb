{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Building a Simple Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will build a simple search index, which we will use later for Boolean retrieval. The assignment tasks are again at the bottom of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summaries_file = 'data/emotion_Summaries.pkl.bz2'\n",
    "Abstracts_file = 'data/emotion_Abstracts.pkl.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, bz2\n",
    "from collections import namedtuple\n",
    "\n",
    "Summaries = pickle.load( bz2.BZ2File( Summaries_file, 'rb' ) )\n",
    "\n",
    "paper = namedtuple( 'paper', ['title', 'authors', 'year', 'doi'] )\n",
    "\n",
    "for (id, paper_info) in Summaries.items():\n",
    "    Summaries[id] = paper( *paper_info )\n",
    "    \n",
    "Abstracts = pickle.load( bz2.BZ2File( Abstracts_file, 'rb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what the data looks like for an example of a paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper(title='Electrophysiological correlates of interference control in the modified emotional Stroop task with emotional stimuli differing in valence, arousal, and subjective significance.', authors=['Imbir KK', 'Pastwa M', 'Duda-Goławska J', 'Sobieszek A', 'Jankowska M', 'Modzelewska A', 'Wielgopolan A', 'Żygierewicz J'], year=2021, doi='10.1371/journal.pone.0258177')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summaries[34648542]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We present the Amsterdam Open MRI Collection (AOMIC): three datasets with multimodal (3\\u2009T) MRI data including structural (T1-weighted), diffusion-weighted, and (resting-state and task-based) functional BOLD MRI data, as well as detailed demographics and psychometric variables from a large set of healthy participants (N\\u2009=\\u2009928, N\\u2009=\\u2009226, and N\\u2009=\\u2009216). Notably, task-based fMRI was collected during various robust paradigms (targeting naturalistic vision, emotion perception, working memory, face perception, cognitive conflict and control, and response inhibition) for which extensively annotated event-files are available. For each dataset and data modality, we provide the data in both raw and preprocessed form (both compliant with the Brain Imaging Data Structure), which were subjected to extensive (automated and manual) quality control. All data is publicly available from the OpenNeuro data sharing platform.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstracts[33741990]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define some utility functions that allow us to tokenize a string into terms, perform linguistic preprocessing on a list of terms, as well as a function to display information about a paper in a nice way. Note that these tokenization and preprocessing functions are rather naive. We will improve them in a later assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', 'dolor', 'sit', 'amet']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function that tokenizes a string in a rather naive way. Can be extended later.\n",
    "    \"\"\"\n",
    "    return text.split(' ')\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\"\n",
    "    Perform linguistic preprocessing on a list of tokens. Can be extended later.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.append(token.lower())\n",
    "    return result\n",
    "\n",
    "print(preprocess(tokenize(\"Lorem ipsum dolor sit AMET\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0258177>Electrophysiological correlates of interference control in the modified emotional Stroop task with emotional stimuli differing in valence, arousal, and subjective significance.</a></strong><br>2021. Imbir KK, Pastwa M, Duda-Goławska J, Sobieszek A, Jankowska M, Modzelewska A, Wielgopolan A, Żygierewicz J<br>[ID: 34648542]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1371/journal.pone.0258177>Electrophysiological correlates of interference control in the modified emotional Stroop task with emotional stimuli differing in valence, arousal, and subjective significance.</a></strong><br>2021. Imbir KK, Pastwa M, Duda-Goławska J, Sobieszek A, Jankowska M, Modzelewska A, Wielgopolan A, Żygierewicz J<br><small><strong>Abstract:</strong> <em>The role of emotional factors in maintaining cognitive control is one of the most intriguing issues in understanding emotion-cognition interactions. In the current experiment, we assessed the role of emotional factors (valence, arousal, and subjective significance) in perceptual and conceptual inhibition processes. We operationalised both processes with the classical cognitive paradigms, i.e., the flanker task and the emotional Stroop task merged into a single experimental procedure. The procedure was based on the presentation of emotional words displayed in four different font colours flanked by the same emotional word printed with the same or different font colour. We expected to find distinct effects of both types of interference: earlier for perceptual and later for emotional interference. We also predicted an increased arousal level to disturb inhibitory control effectiveness, while increasing the subjective significance level should improve this process. As we used orthogonal manipulations of emotional factors, our study allowed us for the first time to assess interactions within emotional factors and between types of interference. We found on the behavioural level the main effects of flanker congruency as well as effects of emotionality. On the electrophysiological level, we found effects for EPN, P2, and N450 components of ERPs. The exploratory analysis revealed that effects due to perceptual interference appeared earlier than the effects of emotional interference, but they lasted for an extended period of processing, causing perceptual and emotional interference to partially overlap. Finally, in terms of emotional interference, we showed the effect of subjective significance: the reduction of interference cost in N450 for highly subjective significant stimuli. This study is the first one allowing for the investigation of two different types of interference in a single experiment, and provides insight into the role of emotion in cognitive control.</em></small><br>[ID: 34648542]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "def display_summary( id, show_abstract=False, show_id=True, extra_text='' ):\n",
    "    \"\"\"\n",
    "    Function for printing a paper's summary through IPython's Rich Display System.\n",
    "    Trims long author lists, and adds a link to the paper's DOI (when available).\n",
    "    \"\"\"\n",
    "    s = Summaries[id]\n",
    "    lines = []\n",
    "    title = s.title\n",
    "    if s.doi != '':\n",
    "        title = '<a href=http://dx.doi.org/{:s}>{:s}</a>'.format(s.doi, title)\n",
    "    title = '<strong>' + title + '</strong>'\n",
    "    lines.append(title)\n",
    "    authors = ', '.join( s.authors[:20] ) + ('' if len(s.authors) <= 20 else ', ...')\n",
    "    lines.append(str(s.year) + '. ' + authors)\n",
    "    if (show_abstract):\n",
    "        lines.append('<small><strong>Abstract:</strong> <em>{:s}</em></small>'.format(Abstracts[id]))\n",
    "    if (show_id):\n",
    "        lines.append('[ID: {:d}]'.format(id))\n",
    "    if (extra_text != ''):\n",
    "         lines.append(extra_text)\n",
    "    display( HTML('<br>'.join(lines)) )\n",
    "\n",
    "display_summary(34648542)\n",
    "display_summary(34648542, show_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our first index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create an _inverted index_ based on the words in the titles and abstracts of the papers in our dataset. We will implement our inverted index as a Python dictionary with term strings as keys and posting lists (implemented as Python lists) as values. We include all the tokens we can find in the title and (if available) in the abstract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(list)\n",
    "\n",
    "# This can take a few seconds:\n",
    "for id in sorted(Summaries.keys()):\n",
    "    term_set = set(preprocess(tokenize(Summaries[id].title)))\n",
    "    if id in Abstracts:\n",
    "        term_set.update(preprocess(tokenize(Abstracts[id])))\n",
    "    for term in term_set:\n",
    "        inverted_index[term].append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in the index for the example term 'amsterdam':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9756244, 16916567, 21859206, 25186285, 26784347, 29218587, 29406610, 33741990]\n"
     ]
    }
   ],
   "source": [
    "print(inverted_index['amsterdam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this inverted index to answer simple one-word queries, for example to show all papers that contain the word 'utrecht':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inverted_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b50e30efeb93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mquery_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utrecht'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minverted_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdisplay_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_abstract\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inverted_index' is not defined"
     ]
    }
   ],
   "source": [
    "query_word = 'utrecht'\n",
    "for i in inverted_index[query_word]:\n",
    "    display_summary(i, show_abstract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:** Astha Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Implement the function `and_merge` outlined below. This function takes two posting lists from the index that can be assumed to be sorted already, and it should return the result of the merging of the two lists with AND. The resulting list should therefore include all the elements that appear in both lists. As explained on the slides, this operation should take advantage of the input lists being sorted already, should not perform any additional sorting operation, and should go through each of the input lists just once. Then, test your function with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 7, 2, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "def and_merge(sorted_list1, sorted_list2):\n",
    "    merged_list = []\n",
    "    # first we make copies of the lists, so we don't modify the existing lists in the index:\n",
    "    list1 = list(sorted_list1)\n",
    "    list2 = list(sorted_list2)\n",
    "    for i in list1:\n",
    "        for j in list2:\n",
    "            if i == j and i not in merged_list:\n",
    "                merged_list.append(i)\n",
    "    return merged_list\n",
    "\n",
    "# testing\n",
    "lst1 = [1,6,7,1,4,6,2,5,5,8]\n",
    "lst2 = [1,5,7,1,3,6,2,3,6,8]\n",
    "print(and_merge(lst1,lst2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Similarly as above, implement the function `or_merge` outlined below that executes an OR merging of the lists. The resulting list should therefore include all the elements that appear in at least one of the lists. Again, this operation should take advantage of the input lists being sorted already, should not perform any additional sorting operation, and should go through each of the input lists just once. Elements that appear in both input list should only appear once in the output list. Test your function again with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "def or_merge(sorted_list1, sorted_list2):\n",
    "    merged_list = []\n",
    "    # first we make copies of the lists, so we don't modify the existing lists in the index:\n",
    "    list1 = list(sorted_list1)\n",
    "    list2 = list(sorted_list2)\n",
    "    list3 = list1+list2\n",
    "    merged_list = list(dict.fromkeys(list3))\n",
    "    return merged_list\n",
    "\n",
    "# testing:\n",
    "lst1 = [1,2,3,3]\n",
    "lst2 = [4,5,6,6]\n",
    "print(or_merge(lst1,lst2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Construct a function called `and_query` that takes as input a single string, consisting of one or more words, and returns as function value a list of matching documents. `and_query`, as its name suggests, should require that all query terms are present in the documents of the result list.\n",
    "\n",
    "For that, access the variable `inverted_index` from above and use the method `and_merge` that you defined. Also use the `tokenize` and `preprocess` functions we defined above to tokenize and preprocess your query.\n",
    "\n",
    "Again demonstrate the working of your function with an example (choose one that leads to fewer than 100 hits to not overblow this notebook file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34410788]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def and_query(string):\n",
    "    l1 = preprocess(tokenize(string))\n",
    "    r = []\n",
    "    for i in l1:\n",
    "        if l1.index(i) != 0:\n",
    "    r = and_merge(r, inverted_index[i])\n",
    "        else:\n",
    "            r = inverted_index[i]\n",
    "    return r\n",
    "    \n",
    "        \n",
    "and_query(\"automated quality control\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Construct another function called `or_query` that works in the same way as `and_query` you just implemented, but returns as function value the documents that contain _at least one_ of the words in the query, using the `or_merge` function you defined.\n",
    "\n",
    "Demonstrate the working of this function also with an example (again, choose one that leads to fewer than 100 hits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33488995,\n",
       " 25425012,\n",
       " 3997585,\n",
       " 7270111,\n",
       " 7690981,\n",
       " 9112614,\n",
       " 9119582,\n",
       " 9309949,\n",
       " 9589285,\n",
       " 11419222,\n",
       " 12477605,\n",
       " 12887631,\n",
       " 16414281,\n",
       " 17180160,\n",
       " 18000554,\n",
       " 19221452,\n",
       " 19485687,\n",
       " 20346609,\n",
       " 20515223,\n",
       " 20733344,\n",
       " 21610856,\n",
       " 21779754,\n",
       " 21924368,\n",
       " 21934100,\n",
       " 22309729,\n",
       " 22457894,\n",
       " 22557958,\n",
       " 23055158,\n",
       " 24065950,\n",
       " 24495430,\n",
       " 24920615,\n",
       " 25064061,\n",
       " 25297056,\n",
       " 25429141,\n",
       " 25806706,\n",
       " 25816373,\n",
       " 25891321,\n",
       " 26558785,\n",
       " 26651622,\n",
       " 26778369,\n",
       " 27318957,\n",
       " 27378986,\n",
       " 28668136,\n",
       " 28776466,\n",
       " 29170615,\n",
       " 29258849,\n",
       " 29541041,\n",
       " 29675706,\n",
       " 29720691,\n",
       " 29741242,\n",
       " 30120839,\n",
       " 30649979,\n",
       " 30661754,\n",
       " 30919792,\n",
       " 30974241,\n",
       " 31016215,\n",
       " 31153307,\n",
       " 31288695,\n",
       " 31993472,\n",
       " 32107980,\n",
       " 32126893,\n",
       " 32416601,\n",
       " 32530313,\n",
       " 32697723,\n",
       " 32962172,\n",
       " 33159987,\n",
       " 33177977,\n",
       " 33609185,\n",
       " 33672413,\n",
       " 33928872,\n",
       " 34055236]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def or_query(string):\n",
    "    l1 = preprocess(tokenize(string))\n",
    "    r = []\n",
    "    for i in l1:\n",
    "        if l1.index(i) != 0:\n",
    "            r = or_merge(r, inverted_index[i])\n",
    "        else:\n",
    "            r = inverted_index[i]\n",
    "    return r\n",
    "    \n",
    "        \n",
    "or_query('ASS @ CODE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Why does `and_query('emotional factor experiment')` not return our example paper 34648542, even though it mentions emotional factors and experiments in the abstract? (You do not have to implement anything to fix this yet!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Expressed emotion (EE), a measure of the family's emotional climate, is a fundamental measure in caregiving research. A core dimension of EE is the level of criticism expressed by the caregiver to the care recipient, with a high level of criticism a marker of significant distress in the household. The Five-Minute Speech Sample (FMSS), the most commonly used brief measure of EE, requires time-consuming manual processing and scoring by a highly trained expert. In this study, we used natural language processing and supervised machine learning techniques to develop a fully automated framework to evaluate caregiver criticism level based on the verbatim transcript of the FMSS. The success of the machine learning algorithm was established by demonstrating that the classification of maternal caregivers as high versus low EE was consistent with the classification of these 298 maternal caregivers of adult children with schizophrenia using standard manual coding procedures, with area under the receiver operating characteristic curve (AUROC) of 0.76. Evidence of construct validity was established by demonstrating that maternal caregivers of adults with schizophrenia, who were classified as having a high level of criticism had higher levels of caregiver burden, reported that their child had more psychiatric symptoms and behaviors and perceived that their child had greater control over these symptoms and behaviors. Additionally, maternal caregivers who had high levels of criticism reported having a poorer quality of relationship with their child with schizophrenia than maternal caregivers low on criticism. Rapid measurement of criticism facilitates the incorporation of this dimension into research across a broad range of caregiving contexts. (PsycInfo Database Record (c) 2021 APA, all rights reserved).\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstracts[34410788]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** [The only noticeable difference between the query and abstract is that the word factor is only present in the plural form. Therefore, 'factor' explcitly does not exist within the abstract.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the answers to the assignment via Canvas as a modified version of this Notebook file (file with `.ipynb` extension) that includes your code and your answers.\n",
    "\n",
    "Before submitting, restart the kernel and re-run the complete code (**Kernel > Restart & Run All**), and then check whether your assignment code still works as expected.\n",
    "\n",
    "Don't forget to add your name, and remember that the assignments have to be done **individually**, and that code sharing or copying are **strictly forbidden** and will be punished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
